name: "Scraping & Data Extraction Engine"

on:
  workflow_call:
    inputs:
      task-id:
        required: true
        type: string
      target-url:
        required: true
        type: string
      scraping-mode:
        required: false
        type: string
        default: 'smart'
      output-format:
        required: false
        type: string
        default: 'json'
      depth-limit:
        required: false
        type: string
        default: '1'
  workflow_dispatch:
    inputs:
      task-id:
        description: 'Task ID for the scraping job'
        required: true
        type: string
      target-url:
        description: 'URL to scrape'
        required: true
        type: string
      scraping-mode:
        description: 'Scraping mode'
        required: false
        type: choice
        default: 'smart'
        options:
        - smart
        - basic
        - playwright
      output-format:
        description: 'Output format'
        required: false
        type: choice
        default: 'json'
        options:
        - json
        - csv
      depth-limit:
        description: 'Depth limit for scraping'
        required: false
        type: choice
        default: '1'
        options:
        - '1'
        - '2'
        - '3'
permissions:
  contents: write
  pull-requests: write
concurrency:
  group: scraping-${{ inputs.task-id }}
  cancel-in-progress: false

jobs:
  scraping-engine:
    name: ðŸŒ Web Scraping Engine
    runs-on: ubuntu-latest
    timeout-minutes: 30
    
    outputs:
      status: ${{ steps.scrape-execution.outputs.status }}
      output-file: ${{ steps.scrape-execution.outputs.output_file }}
      data-summary: ${{ steps.scrape-execution.outputs.data_summary }}
    
    steps:
    - name: ðŸ“¥ Checkout Repository
      uses: actions/checkout@v4

    - name: ðŸ Setup Python Environment
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        cache: 'pip'

    - name: ðŸ› ï¸ Install Scraping Dependencies
      run: |
        pip install --upgrade pip
        pip install requests beautifulsoup4 selenium pandas numpy lxml
        pip install playwright asyncio aiohttp cssselect
        pip install scrapy fake-useragent

        # Install browser for Playwright using Python CLI
        python -m playwright install --with-deps chromium

    - name: ðŸ“ Create Output Directory
      run: |
        mkdir -p outputs/scraping/${{ inputs.task-id }}
        mkdir -p outputs/raw-data/${{ inputs.task-id }}

    - name: ðŸš€ Execute Scraping Task
      id: scrape-execution
      run: |
        cat > scraper.py << 'EOF'
        import requests
        import json
        import pandas as pd
        import asyncio
        import aiohttp
        from bs4 import BeautifulSoup
        from playwright.async_api import async_playwright
        from urllib.parse import urljoin, urlparse
        import re
        import os
        import sys
        from datetime import datetime
        
        class SmartScraper:
            def __init__(self, task_id, target_url, mode='smart', depth=1):
                self.task_id = task_id
                self.target_url = target_url
                self.mode = mode
                self.depth = int(depth)
                self.session = requests.Session()
                self.session.headers.update({
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                })
                self.scraped_data = []
                self.links_found = []
                
            def extract_text_content(self, soup):
                """Extract clean text content from BeautifulSoup object"""
                # Remove script and style elements
                for script in soup(["script", "style"]):
                    script.extract()
                
                # Extract text
                text = soup.get_text()
                
                # Clean up text
                lines = (line.strip() for line in text.splitlines())
                chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
                text = ' '.join(chunk for chunk in chunks if chunk)
                
                return text
            
            def extract_structured_data(self, soup, url):
                """Extract structured data from webpage"""
                data = {
                    'url': url,
                    'title': soup.title.string if soup.title else '',
                    'timestamp': datetime.now().isoformat(),
                    'content': {},
                    'metadata': {}
                }
                
                # Extract headings
                headings = []
                for i in range(1, 7):
                    headings.extend([h.get_text().strip() for h in soup.find_all(f'h{i}')])
                data['content']['headings'] = headings
                
                # Extract paragraphs
                paragraphs = [p.get_text().strip() for p in soup.find_all('p')]
                data['content']['paragraphs'] = paragraphs
                
                # Extract lists
                lists = []
                for ul in soup.find_all(['ul', 'ol']):
                    items = [li.get_text().strip() for li in ul.find_all('li')]
                    lists.extend(items)
                data['content']['lists'] = lists
                
                # Extract tables
                tables = []
                for table in soup.find_all('table'):
                    table_data = []
                    for row in table.find_all('tr'):
                        row_data = [cell.get_text().strip() for cell in row.find_all(['td', 'th'])]
                        table_data.append(row_data)
                    if table_data:
                        tables.append(table_data)
                data['content']['tables'] = tables
                
                # Extract links
                links = []
                for a in soup.find_all('a', href=True):
                    link_url = urljoin(url, a['href'])
                    link_text = a.get_text().strip()
                    if link_text and link_url.startswith('http'):
                        links.append({'url': link_url, 'text': link_text})
                data['content']['links'] = links[:50]  # Limit to 50 links
                
                # Extract images
                images = []
                for img in soup.find_all('img', src=True):
                    img_url = urljoin(url, img['src'])
                    img_alt = img.get('alt', '')
                    images.append({'url': img_url, 'alt': img_alt})
                data['content']['images'] = images[:20]  # Limit to 20 images
                
                # Extract metadata
                for meta in soup.find_all('meta'):
                    if meta.get('name'):
                        data['metadata'][meta['name']] = meta.get('content', '')
                    elif meta.get('property'):
                        data['metadata'][meta['property']] = meta.get('content', '')
                
                # Text content
                data['content']['full_text'] = self.extract_text_content(soup)
                
                return data
            
            async def scrape_with_playwright(self, url):
                """Scrape JavaScript-heavy sites with Playwright"""
                async with async_playwright() as p:
                    browser = await p.chromium.launch()
                    page = await browser.new_page()
                    
                    try:
                        await page.goto(url, wait_until='networkidle')
                        content = await page.content()
                        soup = BeautifulSoup(content, 'html.parser')
                        data = self.extract_structured_data(soup, url)
                        
                        # Extract additional JavaScript-generated content
                        js_data = {}
                        
                        # Try to extract JSON-LD structured data
                        try:
                            json_ld = await page.evaluate('''
                                () => {
                                    const scripts = document.querySelectorAll('script[type="application/ld+json"]');
                                    return Array.from(scripts).map(script => JSON.parse(script.textContent));
                                }
                            ''')
                            if json_ld:
                                js_data['json_ld'] = json_ld
                        except:
                            pass
                        
                        # Extract dynamic content
                        try:
                            dynamic_text = await page.evaluate('() => document.body.innerText')
                            js_data['dynamic_text'] = dynamic_text[:5000]  # Limit text length
                        except:
                            pass
                        
                        data['javascript_content'] = js_data
                        
                    except Exception as e:
                        print(f"Error scraping {url} with Playwright: {e}")
                        return None
                    finally:
                        await browser.close()
                
                return data
            
            def scrape_basic(self, url):
                """Basic scraping with requests"""
                try:
                    response = self.session.get(url, timeout=30)
                    response.raise_for_status()
                    
                    soup = BeautifulSoup(response.content, 'html.parser')
                    return self.extract_structured_data(soup, url)
                    
                except Exception as e:
                    print(f"Error scraping {url}: {e}")
                    return None
            
            async def run_scraping(self):
                """Main scraping execution"""
                print(f"Starting scraping task: {self.task_id}")
                print(f"Target URL: {self.target_url}")
                print(f"Mode: {self.mode}, Depth: {self.depth}")
                
                urls_to_scrape = [self.target_url]
                scraped_urls = set()
                
                for current_depth in range(self.depth):
                    if not urls_to_scrape:
                        break
                        
                    current_urls = urls_to_scrape.copy()
                    urls_to_scrape = []
                    
                    for url in current_urls:
                        if url in scraped_urls:
                            continue
                            
                        print(f"Scraping (depth {current_depth + 1}): {url}")
                        
                        # Choose scraping method based on mode
                        if self.mode == 'playwright' or (self.mode == 'smart' and 'spa' in url.lower()):
                            data = await self.scrape_with_playwright(url)
                        else:
                            data = self.scrape_basic(url)
                        
                        if data:
                            self.scraped_data.append(data)
                            scraped_urls.add(url)
                            
                            # Extract links for next depth level
                            if current_depth < self.depth - 1:
                                for link in data['content']['links']:
                                    link_url = link['url']
                                    # Only add links from same domain
                                    if urlparse(link_url).netloc == urlparse(self.target_url).netloc:
                                        if link_url not in scraped_urls:
                                            urls_to_scrape.append(link_url)
                
                return self.scraped_data
            
            def save_results(self, output_format='json'):
                """Save scraping results"""
                if not self.scraped_data:
                    return None
                
                base_path = f"outputs/scraping/{self.task_id}"
                
                if output_format == 'json':
                    output_file = f"{base_path}/scraped_data.json"
                    with open(output_file, 'w', encoding='utf-8') as f:
                        json.dump(self.scraped_data, f, ensure_ascii=False, indent=2)
                
                elif output_format == 'csv':
                    output_file = f"{base_path}/scraped_data.csv"
                    
                    # Flatten data for CSV
                    flattened_data = []
                    for item in self.scraped_data:
                        flat_item = {
                            'url': item['url'],
                            'title': item['title'],
                            'timestamp': item['timestamp'],
                            'headings_count': len(item['content']['headings']),
                            'paragraphs_count': len(item['content']['paragraphs']),
                            'links_count': len(item['content']['links']),
                            'images_count': len(item['content']['images']),
                            'text_length': len(item['content']['full_text']),
                            'full_text_preview': item['content']['full_text'][:500]
                        }
                        flattened_data.append(flat_item)
                    
                    df = pd.DataFrame(flattened_data)
                    df.to_csv(output_file, index=False)
                
                # Always save detailed JSON for processing
                detailed_file = f"{base_path}/detailed_data.json"
                with open(detailed_file, 'w', encoding='utf-8') as f:
                    json.dump(self.scraped_data, f, ensure_ascii=False, indent=2)
                
                # Generate summary
                summary = {
                    'task_id': self.task_id,
                    'target_url': self.target_url,
                    'total_pages': len(self.scraped_data),
                    'total_text_length': sum(len(item['content']['full_text']) for item in self.scraped_data),
                    'total_links': sum(len(item['content']['links']) for item in self.scraped_data),
                    'total_images': sum(len(item['content']['images']) for item in self.scraped_data),
                    'completion_time': datetime.now().isoformat()
                }
                
                summary_file = f"{base_path}/summary.json"
                with open(summary_file, 'w') as f:
                    json.dump(summary, f, indent=2)
                
                return output_file, summary
        
        # Execute scraping
        async def main():
            scraper = SmartScraper(
                task_id="${{ inputs.task-id }}",
                target_url="${{ inputs.target-url }}",
                mode="${{ inputs.scraping-mode }}",
                depth="${{ inputs.depth-limit }}"
            )
            
            results = await scraper.run_scraping()
            
            if results:
                output_file, summary = scraper.save_results("${{ inputs.output-format }}")
                print(f"Scraping completed successfully!")
                print(f"Output file: {output_file}")
                print(f"Summary: {json.dumps(summary, indent=2)}")
                
                # Set GitHub Action outputs
                gh_out = os.getenv('GITHUB_OUTPUT')
                if gh_out:
                    with open(gh_out, 'a') as f:
                        f.write('status=success\n')
                        f.write(f'output_file={output_file}\n')
                        f.write(f'data_summary={json.dumps(summary)}\n')
            else:
                print("Scraping failed - no data extracted")
                gh_out = os.getenv('GITHUB_OUTPUT')
                if gh_out:
                    with open(gh_out, 'a') as f:
                        f.write('status=failed\n')
        
        if __name__ == "__main__":
            asyncio.run(main())
        EOF
        
        python scraper.py

    - name: ðŸ“Š Generate Scraping Report
      run: |
        echo "## ðŸ•·ï¸ Scraping Task Report" >> $GITHUB_STEP_SUMMARY
        echo "- **Task ID**: ${{ inputs.task-id }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Target URL**: ${{ inputs.target-url }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Mode**: ${{ inputs.scraping-mode }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Depth**: ${{ inputs.depth-limit }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Status**: ${{ steps.scrape-execution.outputs.status }}" >> $GITHUB_STEP_SUMMARY
        
        if [ -f "outputs/scraping/${{ inputs.task-id }}/summary.json" ]; then
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Results Summary:" >> $GITHUB_STEP_SUMMARY
          cat outputs/scraping/${{ inputs.task-id }}/summary.json | jq -r '
            "- **Pages Scraped**: " + (.total_pages | tostring) + "\n" +
            "- **Total Text Length**: " + (.total_text_length | tostring) + " characters\n" +
            "- **Links Found**: " + (.total_links | tostring) + "\n" +
            "- **Images Found**: " + (.total_images | tostring) + "\n" +
            "- **Completion Time**: " + .completion_time
          ' >> $GITHUB_STEP_SUMMARY
        fi

    - name: ðŸ“¤ Upload Scraping Results
      uses: actions/upload-artifact@v4
      if: steps.scrape-execution.outputs.status == 'success'
      with:
        name: scraping-results-${{ inputs.task-id }}
        path: outputs/scraping/${{ inputs.task-id }}/
        retention-days: 30